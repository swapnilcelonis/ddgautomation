name: DDG Uploader (Google Sheets → input.xlsx)

on:
  workflow_dispatch:
    inputs:
      environment:
        description: "Environment that holds YOUR DDG_TOKEN (e.g., env-swapnil)"
        required: true
        type: string
      domain:
        description: "DDG domain incl. protocol (e.g., https://ddg-practice.try.celonis.cloud)"
        required: true
        type: string
        default: "https://ddg-practice.try.celonis.cloud"
      ddgname:
        description: "Dataset name"
        required: true
        type: string
        default: "random11"
      ddgdatapool:
        description: "Data Pool"
        required: true
        type: string
        default: "ddgtest"
      ddgdatamodel:
        description: "Data Model"
        required: true
        type: string
        default: "random1"
      space:
        description: "Space"
        required: true
        type: string
        default: "defaultSpace"
      genpackage:
        description: "Gen Package"
        required: true
        type: string
        default: "defaultPackage"
      analysis:
        description: "Analysis"
        required: true
        type: string
        default: "defaultAnalysis"
      google_sheets_link:
        description: >
          Public Google Sheets link (make sure sharing is 'Anyone with the link').
          Example: https://docs.google.com/spreadsheets/d/<FILE_ID>/edit#gid=0
        required: true
        type: string
      allow_host:
        description: "Safety: only allow API domain hosts ending with this suffix"
        required: false
        type: string
        default: ".celonis.cloud"

permissions:
  contents: read

jobs:
  ddg:
    # bind to the chosen environment so this job sees ONLY that env's secrets
    environment: ${{ inputs.environment }}
    runs-on: ubuntu-latest
    timeout-minutes: 30
    concurrency:
      group: ddg-${{ inputs.environment }}
      cancel-in-progress: false

    steps:
      - uses: actions/checkout@v4

      - name: Validate inputs & mask
        shell: bash
        run: |
          set -eu

          DOMAIN="${{ inputs.domain }}"
          [[ "$DOMAIN" =~ ^https?:// ]] || { echo "Domain must start with http(s)://"; exit 1; }

  
          # Mask a few values in logs
          echo "::add-mask::${{ inputs.domain }}"
          echo "::add-mask::${{ inputs.ddgname }}"
          echo "::add-mask::${{ inputs.ddgdatapool }}"
          echo "::add-mask::${{ inputs.ddgdatamodel }}"

      - name: Setup Node.js 22.17.0
        uses: actions/setup-node@v4
        with:
          node-version: '22.17.0'

      - name: npm install
        run: npm install

      - name: Download Google Sheet as input.xlsx (root)
        shell: bash
        run: |
          set -euo pipefail
          raw="${{ inputs.google_sheets_link }}"
          if [[ -z "$raw" ]]; then
            echo "No Google Sheet link provided"; exit 1
          fi

          # Normalize to a direct XLSX export URL if it's a standard Sheets link
          norm_url () {
            local url="$1"
            if [[ "$url" =~ ^https://docs\.google\.com/spreadsheets/d/([^/]+) ]]; then
              local id="${BASH_REMATCH[1]}"
              echo "https://docs.google.com/spreadsheets/d/${id}/export?format=xlsx"
            else
              # If caller already supplied a direct XLSX URL, pass through
              echo "$url"
            fi
          }

          url="$(norm_url "$raw")"
          echo "Downloading $url -> input.xlsx"
          curl -fsSL "$url" -o input.xlsx

          # Basic sanity checks (non-empty + XLSX zip magic)
          test -s input.xlsx || { echo "input.xlsx is empty"; exit 1; }
          head -c 4 input.xlsx | od -An -t x1
          ls -lh input.xlsx

      # ===== Your JS sequence (1 → 9) =====
      - name: Run entities.js
        run: node entities.js

      - name: Run variants.js
        run: node variants.js

      - name: Run caseTable.js
        run: node caseTable.js

      - name: Run eventObjects.js
        run: node eventObjects.js

      - name: Run objectObjects.js
        run: node objectObjects.js

      - name: Run attributeObjects.js
        run: node attributeObjects.js

      # ===== External API calls =====
      - name: Resolve or create dataset (API #1)
        id: create
        env:
          DOMAIN:   ${{ inputs.domain }}
          TOKEN:    ${{ secrets.DDG_TOKEN }}
          DDGNAME:  ${{ inputs.ddgname }}
          DATA_POOL:  ${{ inputs.ddgdatapool }}
          DATA_MODEL: ${{ inputs.ddgdatamodel }}
          SPACE:      ${{ inputs.space }}
          GENPACKAGE: ${{ inputs.genpackage }}
          ANALYSIS:   ${{ inputs.analysis }}
        shell: bash
        run: |
          set -euo pipefail

          # 1) Try to find an existing dataset by name
          LIST=$(curl -sS -X GET "$DOMAIN/demo-data-generator/api/demo-data-set" \
            -H "accept: application/json" \
            -H "Authorization: Bearer $TOKEN")
          echo "$LIST" > list.json || true

          # Extract first matching id where .name == $DDGNAME
          EXISTING_ID=$(jq -r --arg n "$DDGNAME" '
            ( .[]? // [] ) | map(select(.name == $n)) | .[0].id // empty
          ' list.json 2>/dev/null || true)

          if [[ -n "$EXISTING_ID" ]]; then
            echo "dataset already exists, updating the same (ID: $EXISTING_ID)"
            echo "id=$EXISTING_ID" >> $GITHUB_OUTPUT
            echo "::add-mask::$EXISTING_ID"
            exit 0
          fi

          # 2) No match → create a new dataset
          BODY=$(jq -n \
            --arg name "$DDGNAME" \
            --arg dp "$DATA_POOL" \
            --arg dm "$DATA_MODEL" \
            --arg sp "$SPACE" \
            --arg gp "$GENPACKAGE" \
            --arg an "$ANALYSIS" \
            '{name:$name, ddgType:"OBJECT_CENTRIC", params:{dataPool:$dp, dataModel:$dm, space:$sp, genpackage:$gp, analysis:$an}}')

          RES=$(curl -sS -w '\n%{http_code}' -X POST "$DOMAIN/demo-data-generator/api/demo-data-set" \
            -H "accept: application/json" \
            -H "content-type: application/json" \
            -H "Authorization: Bearer $TOKEN" \
            --data "$BODY")

          CODE="${RES##*$'\n'}"
          BODY_ONLY="${RES%$'\n'*}"
          echo "$BODY_ONLY" > create.json
          echo "HTTP_CODE=$CODE"
          if [[ "$CODE" -lt 200 || "$CODE" -ge 300 ]]; then
            echo "Create dataset failed ($CODE)"
            head -c 2000 create.json || true
            exit 1
          fi

          NEW_ID=$(jq -r '.id // empty' create.json)
          if [[ -z "$NEW_ID" ]]; then
            echo "Missing id in response"; head -c 1000 create.json; exit 1
          fi

          echo "New dataset created (ID: $NEW_ID)"
          echo "id=$NEW_ID" >> $GITHUB_OUTPUT
          echo "::add-mask::$NEW_ID"

      # after Resolve or create dataset (API #1)
      
      - name: Run ddgjsoncreator.js → output.json
        env:
          NEW_ID: ${{ steps.create.outputs.id }}
        run: |
          # (optional) confirm it's set without leaking: print first 8 chars
          echo "Using dataset id: ${NEW_ID:0:8}********"
          node ddgjsoncreator.js \
            --name "${{ inputs.ddgname }}" \
            --dataPool "${{ inputs.ddgdatapool }}" \
            --dataModel "${{ inputs.ddgdatamodel }}" \
            --entities entities.json \
            --variants variants.json \
            --caseTable caseTable.json \
            --eventObjects eventObjects.json \
            --objectObjects objectObjects.json \
            --attributeObjects attributeObjects.json \
            --out output.json
      
      - name: Run changer.js (produces final.json at root)
        env:
          NEW_ID: ${{ steps.create.outputs.id }}
        run: node changer.js
      
      - name: Verify final.json exists
        run: |
          test -s final.json || { echo "final.json not found or empty"; ls -lah; exit 1; }
          echo "final.json size: $(wc -c < final.json) bytes"


      - name: Upload config (API #2) — verbose & safe
        id: upload
        env:
          DOMAIN: ${{ inputs.domain }}
          TOKEN:  ${{ secrets.DDG_TOKEN }}
          ID:     ${{ steps.create.outputs.id }}
        shell: bash
        run: |
          set -euo pipefail

          # Files for diagnostics (will be sanitized before printing/uploading)
          REQ_LOG="curl_upload_debug.txt"
          RES_HEADERS="upload.headers"
          RES_BODY="upload.body"

          # Do the upload with verbose logs redirected to a file
          # -sS: silent but show errors
          # -v: verbose to STDERR -> file
          # -D: dump response headers
          # --output: save response body (success or error) to file
          HTTP_CODE=$(
            curl -sS -v -X POST \
              "$DOMAIN/demo-data-generator/api/demo-data-set/${ID}/config-file/upload" \
              -H "accept: application/json" \
              -H "Authorization: Bearer $TOKEN" \
              -F "file=@final.json" \
              -D "$RES_HEADERS" \
              --output "$RES_BODY" \
              -w '%{http_code}' \
              2> "$REQ_LOG"
          )

          # Sanitize the verbose log so token never appears
          sed -i -E 's/(Authorization: Bearer) [A-Za-z0-9._~-]+/\1 ***REDACTED***/Ig' "$REQ_LOG"

          echo "HTTP_CODE=${HTTP_CODE}"
          echo "Response headers (first 30 lines):"
          head -n 30 "$RES_HEADERS" || true

          # Save a trimmed JSON (or text) view for convenience
          # If JSON, pretty-print a small excerpt; otherwise show first bytes
          if command -v jq >/dev/null 2>&1; then
            # best-effort JSON preview, may fail if not JSON
            (jq -r '.' "$RES_BODY" | head -c 2000) || head -c 2000 "$RES_BODY"
          else
            head -c 2000 "$RES_BODY"
          fi
          echo

          # Also store a compact summary JSON used by later steps / artifacts
          jq -n --arg code "$HTTP_CODE" \
                --slurpfile body <(head -c 5000 "$RES_BODY") \
                '{http_code:$code|tonumber, body: ($body[0]|tostring)}' > upload.json || true

          # Fail if non-2xx
          if [[ "$HTTP_CODE" -lt 200 || "$HTTP_CODE" -ge 300 ]]; then
            echo "Config upload failed (${HTTP_CODE}). See artifacts: curl_upload_debug.txt, upload.headers, upload.body"
            exit 1
          fi

          echo "Config upload completed with HTTP ${HTTP_CODE}"
     
      - name: Publish artifacts (final.json + upload diagnostics)
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: ddg-upload-artifacts
          path: |
            final.json
            upload.json
            upload.headers
            upload.body
            curl_upload_debug.txt
          if-no-files-found: ignore
          retention-days: 7

      - name: Done
        run: echo "Upload has been successful"

      - name: Trigger data job (API #3)
        env:
          DOMAIN: ${{ inputs.domain }}
          TOKEN:  ${{ secrets.DDG_TOKEN }}
          ID:     ${{ steps.create.outputs.id }}
        shell: bash
        run: |
          set -euo pipefail

          BODY=$(jq -n --arg id "$ID" '{demoDataSetId:$id}')

          RES=$(curl -sS -w '\n%{http_code}' -X POST \
            "$DOMAIN/demo-data-generator/api/demo-data-job" \
            -H "accept: application/json" \
            -H "content-type: application/json" \
            -H "Authorization: Bearer $TOKEN" \
            --data "$BODY")

          CODE="${RES##*$'\n'}"
          BODY_ONLY="${RES%$'\n'*}"
          echo "$BODY_ONLY" > job.json
          echo "HTTP_CODE=$CODE"

          if [[ "$CODE" -lt 200 || "$CODE" -ge 300 ]]; then
            echo "Data job trigger failed ($CODE)"
            head -c 2000 job.json || true
            exit 1
          fi

          JOB_ID=$(jq -r '.id // empty' job.json || true)
          echo "Data job triggered (Job ID: $JOB_ID)"

